# General arguments
#model_location: '/pub/bjvela/GLA-GCN/gla_common'
dataset: 'h36m'
keypoints: 'gt'
subjects_train: 'S1,S5,S6,S7,S8'
subjects_test: 'S9,S11'
actions: '*'
checkpoint: 'checkpoint'
resume: ''
evaluate: 'GLA-GCN_oneFrame_jit.pt' 
render: false
by_subject: false
export_training_curves: false
model_name: 'glagcn'

# Model Type Based on Input and Output
flattened_coordinate_model: false
non_video_model: false
video_model: true 
zero_center_root: true

path_to_dataset: '/dfs9/whayes/preserve/H36M_Dataset/h36m'
data_dir: '/pub/bjvela/PoseLab3D/data'


num_workers: 2                  # Number of workers for data loading


# Dataset args 
remove_global_offset: true
keep_traj_first_pos: true

# Input arguments
input_shape:
  batch_size: 512
  num_frames: 1        # Temporal frames for sequence processing
  num_joints: 17
  coordinates: 2         # 2D input coordinates (x, y)

# Output arguments
output_shape:
  batch_size: 512
  num_frames: 1          # Single-frame predictions
  num_joints: 17
  coordinates: 3         # 3D output coordinates (x, y, z)

# Model arguments
stride: 1
epochs: 80
dropout: 0.1
learning_rate: 0.01
lr_decay: 0.95
data_augmentation: false
test_time_augmentation: false
architecture: '1'
causal: false
channels: 96
number_of_frames: 1

# Experimental arguments
subset: 1
downsample: 1
# no_eval: false
# dense: false
# disable_optimizations: false
# linear_projection: false
# bone_length_term: true
# no_proj: false
# debug: false

# # Visualization
# viz_subject: null
# viz_action: null
# viz_camera: 0
# viz_video: null
# viz_skip: 0
# viz_output: null
# viz_bitrate: 3000
# viz_no_ground_truth: false
# viz_limit: -1
# viz_downsample: 1
# viz_size: 5